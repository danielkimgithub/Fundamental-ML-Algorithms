---
title: "HW3"
output: pdf_document
date: "2024-10-14"
---

# HW 3 - DSC 441

## Problem 1

For this problem, you will perform a straightforward training and evaluation of a decision tree, as well as generate rules by hand. Load the breast_cancer_updated.csv data. These data are visual features computed from samples of breast tissue being evaluated for cancer. As a preprocessing step, *remove the IDNumber column* and *exclude rows with NA from the dataset*.

```{r load}
library(tidyverse)

cancer <- read_csv("/Users/danielkim/Downloads/breast_cancer_updated.csv")
head(cancer)
dim(cancer)

cancer <- cancer %>% select(-c("IDNumber")) %>% drop_na()
summary(cancer)
dim(cancer)

```

-   a: Apply decision tree learning (use rpart) to the data to predict breast cancer malignancy (Class) and report the accuracy using 10-fold cross validation.

```{r decision tree}
library(caret)
library(lattice)

train_control <- trainControl(method = 'cv', number = 10)

model_tree <- train(Class ~., data = cancer, method = 'rpart', trControl = train_control)

model_tree
```

The accuracy for the optimal model was 93.84%, which had a corresponding complexity parameter value of 0.0251.

```{r tree predictions}

tree_predictions <- predict(model_tree, cancer)

confusionMatrix(as.factor(cancer$Class), tree_predictions)

```

The predictions made on the testing set using the trained model earned an accuracy of 94.58% with a statistically significant p-value, rejecting the null hypothesis which stated there was no relationship between the independent and dependent variables. The no information rate, for which it describes the proportion of the majority class, was 64.57%.

-   b: Generate a visualization of the decision tree.

```{r decision visuals}
library(rattle)

attributes(model_tree)

fancyRpartPlot(model_tree$finalModel, caption = "Decision Tree for Breast Tissue Classification")

```

-   c: Generate the full set of rules using IF-THEN statements.

```{r set of rules}

model_tree$finalModel

```

-   If the breast tissue has a uniform cell size that is less than 2.5 mm,

    -   then the breast tissue cell is a benign tumor.

-   If the breast tissue has a uniform cell size that is greater than or equal to 2.5 mm

    -   and has a uniform cell shape of less than 2.5 mm,

        -   then the breast tissue cell is a benign tumor.

    -   and has a uniform cell shape of greater than or equal to 2.5 mm,

        -   then the breast tissue cell is a malignant tumor.

## Problem 2

In this problem you will generate decision trees with a set of parameters. You will be using the storms data, a subset of the NOAA Atlantic hurricane database, which includes the positions and attributes of 198 tropical storms (potential hurricanes), measured every six hours during the lifetime of a storm. It is part of the dplyr library, so load the library and you will be able to access it. As a preprocessing step, view the data and make sure the target variable (category) is converted to a factor (as opposed to character string).

```{r load storms data}
library(dplyr)

data(storms)
dim(storms)

storms$category <- as.factor(storms$category)

head(storms)

# drop rows without classification
storms <- storms %>% drop_na(category)
dim(storms)

# hurricane_force_diameter and tropicalstorm_force_diameter have null values
dim(storms)
summary(storms)

# first drop rows with null values in hurricane_force_diametere
storms <- storms %>% drop_na(hurricane_force_diameter) 
dim(storms)

# confirmed no more null values
sum(is.na(storms))

head(storms)
```

```{r eda}
# drop columns with NA values

# with a sample size of 4803 remaining, dropping 2633 samples with missing values would reduce the sample size too much
# rather keep the sample size and drop columns with missing values especially as many force diameter data is missing for entire storms and it wouldn't be accurate to infer missing force diameter for one storm from the force diameter of a completely different storm with all its sets of conditions

# name was dropped as it's purpose is the ID the specific storm

# status was dropped as all instances were categorized under hurricane once rows without categories were dropped
summary(storms$status)

storms <- storms %>% select(-c("name", "status"))
head(storms)
summary(storms)
```

-   a: Build a decision tree using the following hyperparameters, maxdepth=2, minsplit=5 and minbucket=3. *Be careful to use the right method of training so that you are not automatically tuning the cp parameter, but you are controlling the aforementioned parameters specifically*. Use cross validation to report your accuracy score. These parameters will result in a relatively small tree.

```{r train model}
library(rpart)
# sample method
train_control = trainControl(method = "cv", number = 10)

#change hyper parameters
hypers <- rpart.control(minsplit =  5, maxdepth = 2, minbucket = 3)

# train the model
tree <- train(category ~., data = storms, control = hypers, trControl = train_control, method = "rpart1SE")

tree

```

The accuracy of the decision tree model using cross validation was 83.59%.

-   b: To see how this performed with respect to the individual classes, we could use a confusion matrix. We also want to see if that aspect of performance is different on the train versus the test set.
    1.  Create a train/test partition.
    2.  Train on the training set.
    3.  By making predictions with that model on the train set and on the test set separately, use the outputs to create two separate confusion matrices, one for each partition. Remember, we are testing if the model built with the training data performs differently on data used to train it (train set) as opposed to new data (test set).
    4.  Compare the confusion matrices and report which classes it has problem classifying. Do you think that both are performing similarly and what does that suggest about overfitting for the model?

```{r train/test partition}

# Partition the data
index = createDataPartition(y=storms$category, p=0.7, list=FALSE)
# Everything in the generated index list
train_set = storms[index,]
# Everything except the generated indices
test_set = storms[-index,]


# Train the model with training set
tree1 <- train(category ~., data = train_set, method = "rpart1SE", trControl = train_control)


# Evaluate the fit on training set predictions
pred_tree1 <- predict(tree1, train_set)
# Evaluate the fit on testing set predictions
pred_tree1_2 <- predict(tree1, test_set)


# Confusion Matrix with training set predictions
confusionMatrix(pred_tree1, train_set$category)
# Confusion Matrix with testing set predictions
confusionMatrix(pred_tree1_2, test_set$category)

```

The decision tree did not have problems reporting any category classes. The model that was trained on the training set had the same performance on the training and testing sets, indicating that the model was not over fittting. There would be an issue with over fitting if the model had a better performance with the training set than it did with the testing set, however, as the performance of the model on both sets were the same, there was no over fitting issue.

## Problem 3

-   a: Partition your data into 80% for training and 20% for the test data set

```{r partition}

# Partition the data
part = createDataPartition(y=storms$category, p=0.8, list=FALSE)

# Everything in the generated index list
train_part = storms[index,]
# Everything except the generated indices
test_part = storms[-index,]

```

-   b: Train at least 10 trees using different sets of parameters, through you made need more. Create the graph described above such that you can identify the inflection point where the tree is over fitting and pick a high-quality decision tree. Your strategy should be to make at least one very simple model and at least one very complex model and work towards the center by changing different parameters. Generate a table that contains all of the parameters (maxdepth, minsplit, minbucket, etc) used along with the number of nodes created, and the training and testing set accuracy values. The number of rows will be equal to the number of sets of parameters used. You will use the data in the table to generate the graph. The final results to be reported for this problem are the table and graph.

```{r model 10 trees}

# Initialize cross validation
train_control = trainControl(method = "cv", number = 10)

# Tree 1
hyper1 = rpart.control(minsplit =  2, maxdepth = 1, minbucket = 2)
tree1 <- train(category ~., data = train_part, control = hyper1, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree1 <- predict(tree1, train_part)
# Confusion Matrix
cfm_train1 <- confusionMatrix(train_set$category, pred_tree1)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree1 <- predict(tree1, test_part)
# Confusion Matrix
cfm_test1 <- confusionMatrix(test_part$category, pred_tree1)

# Get training accuracy
acc_train <- cfm_train1$overall[1]
# Get testing accuracy
acc_test <- cfm_test1$overall[1]
# Get number of nodes
nodes <- nrow(tree1$finalModel$frame)

# Form the table
comp_tbl <- data.frame("Nodes" = nodes, "TrainAccuracy" = acc_train, "TestAccuracy" = acc_test,
                       "MaxDepth" = 1, "Minsplit" = 2, "Minbucket" = 2)


# Tree 2
hyper2 = rpart.control(minsplit =  5, maxdepth = 2, minbucket = 5)
tree2 <- train(category ~., data = train_part, control = hyper2, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree2 <- predict(tree2, train_part)
# Confusion Matrix
cfm_train2 <- confusionMatrix(train_part$category, pred_tree2)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree2 <- predict(tree2, test_part)
# Confusion Matrix
cfm_test2 <- confusionMatrix(test_part$category, pred_tree2)

# Get training accuracy
a_train <- cfm_train2$overall[1]
# Get testing accuracy
a_test <- cfm_test2$overall[1]
# Get number of nodes
nodes <- nrow(tree2$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 2, 5, 5))


# Tree 3
hyper3 = rpart.control(minsplit = 50, maxdepth = 3, minbucket = 50)
tree3 <- train(category ~., data = train_part, control = hyper3, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree3 <- predict(tree3, train_part)
# Confusion Matrix
cfm_train3 <- confusionMatrix(train_part$category, pred_tree3)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree3 <- predict(tree3, test_part)
# Confusion Matrix
cfm_test3 <- confusionMatrix(test_part$category, pred_tree3)

# Get training accuracy
a_train <- cfm_train3$overall[1]
# Get testing accuracy
a_test <- cfm_test3$overall[1]
# Get number of nodes
nodes <- nrow(tree3$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 3, 50, 50))


# Tree 9
hyper9 = rpart.control(minsplit = 100, maxdepth = 3, minbucket = 100)
tree9 <- train(category ~., data=train_part, control = hyper9, trControl = train_control, method = 'rpart1SE')

# Training Set
# Evaluate the fit with confusion matrix
pred_tree9 <- predict(tree9, train_part)
# Confusion Matrix
cfm_train9 <- confusionMatrix(train_part$category, pred_tree9)

# Test Set
# Evaluate fit with a confusion matrix
pred_tree9 <- predict(tree9, test_part)
# Confusion Matrix
cfm_test9 <- confusionMatrix(test_part$category, pred_tree9)

#Get training accuracy
a_train <- cfm_train9$overall[1]
#Get testing accuracy
a_test <- cfm_test9$overall[1]
#Get number of nodes
nodes <- nrow(tree9$finalModel$frame)

# Add row to table
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 3, 100, 100))


# Tree 11
hyper11 = rpart.control(minsplit = 1000, maxdepth = 3, minbucket = 1000)
tree11 <- train(category ~., data=train_part, control = hyper11, trControl = train_control, method = 'rpart1SE')

# Training Set
# Evaluate the fit with confusion matrix
pred_tree11 <- predict(tree11, train_part)
# Confusion Matrix
cfm_train11 <- confusionMatrix(train_part$category, pred_tree11)

# Test Set
# Evaluate fit with a confusion matrix
pred_tree11 <- predict(tree11, test_part)
# Confusion Matrix
cfm_test11 <- confusionMatrix(test_part$category, pred_tree11)

#Get training accuracy
a_train <- cfm_train11$overall[1]
#Get testing accuracy
a_test <- cfm_test11$overall[1]
#Get number of nodes
nodes <- nrow(tree11$finalModel$frame)

# Add row to table
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 3, 1000, 1000))


# Tree 12
hyper12 = rpart.control(minsplit = 25, maxdepth = 4, minbucket = 25)
tree12 <- train(category ~., data=train_part, control = hyper12, trControl = train_control, method = 'rpart1SE')

# Training Set
# Evaluate the fit with confusion matrix
pred_tree12 <- predict(tree12, train_part)
# Confusion Matrix
cfm_train12 <- confusionMatrix(train_part$category, pred_tree12)

# Test Set
# Evaluate fit with a confusion matrix
pred_tree12 <- predict(tree12, test_part)
# Confusion Matrix
cfm_test12 <- confusionMatrix(test_part$category, pred_tree12)

#Get training accuracy
a_train <- cfm_train12$overall[1]
#Get testing accuracy
a_test <- cfm_test12$overall[1]
#Get number of nodes
nodes <- nrow(tree12$finalModel$frame)

# Add row to table
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 4, 25, 25))


# Tree 10
hyper10 = rpart.control(minsplit = 50, maxdepth = 4, minbucket = 50)
tree10 <- train(category ~., data=train_part, control = hyper10, trControl = train_control, method = 'rpart1SE')

# Training Set
# Evaluate the fit with confusion matrix
pred_tree10 <- predict(tree10, train_part)
# Confusion Matrix
cfm_train10 <- confusionMatrix(train_part$category, pred_tree10)

# Test Set
# Evaluate fit with a confusion matrix
pred_tree10 <- predict(tree10, test_part)
# Confusion Matrix
cfm_test10 <- confusionMatrix(test_part$category, pred_tree10)

#Get training accuracy
a_train <- cfm_train10$overall[1]
#Get testing accuracy
a_test <- cfm_test10$overall[1]
#Get number of nodes
nodes <- nrow(tree10$finalModel$frame)

# Add row to table
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 4, 50, 50))


# Tree 4
hyper4 = rpart.control(minsplit = 100, maxdepth = 4, minbucket = 100)
tree4 <- train(category ~., data = train_part, control = hyper4, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree4 <- predict(tree4, train_part)
# Confusion Matrix
cfm_train4 <- confusionMatrix(train_part$category, pred_tree4)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree4 <- predict(tree4, test_part)
# Confusion Matrix
cfm_test4 <- confusionMatrix(test_part$category, pred_tree4)

# Get training accuracy
a_train <- cfm_train4$overall[1]
# Get testing accuracy
a_test <- cfm_test4$overall[1]
# Get number of nodes
nodes <- nrow(tree4$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 4, 100, 100))

# Tree 5
hyper5 = rpart.control(minsplit = 1000, maxdepth = 4, minbucket = 1000)
tree5 <- train(category ~., data = train_part, control = hyper5, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree5 <- predict(tree5, train_part)
# Confusion Matrix
cfm_train5 <- confusionMatrix(train_part$category, pred_tree5)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree5 <- predict(tree5, test_part)
# Confusion Matrix
cfm_test5 <- confusionMatrix(test_part$category, pred_tree5)

# Get training accuracy
a_train <- cfm_train5$overall[1]
# Get testing accuracy
a_test <- cfm_test5$overall[1]
# Get number of nodes
nodes <- nrow(tree5$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 4, 1000, 1000))


# Tree 13
hyper13 = rpart.control(minsplit = 25, maxdepth = 5, minbucket = 25)
tree13 <- train(category ~., data = train_part, control = hyper13, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree13 <- predict(tree13, train_part)
# Confusion Matrix
cfm_train13 <- confusionMatrix(train_part$category, pred_tree13)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree13 <- predict(tree13, test_part)
# Confusion Matrix
cfm_test13 <- confusionMatrix(test_part$category, pred_tree13)

# Get training accuracy
a_train <- cfm_train13$overall[1]
# Get testing accuracy
a_test <- cfm_test13$overall[1]
# Get number of nodes
nodes <- nrow(tree13$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 5, 25, 25))

# Tree 7
hyper7 = rpart.control(minsplit = 50, maxdepth = 5, minbucket = 50)
tree7 <- train(category ~., data = train_part, control = hyper7, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree7 <- predict(tree7, train_part)
# Confusion Matrix
cfm_train7 <- confusionMatrix(train_part$category, pred_tree7)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree7 <- predict(tree7, test_part)
# Confusion Matrix
cfm_test7 <- confusionMatrix(test_part$category, pred_tree7)

# Get training accuracy
a_train <- cfm_train7$overall[1]
# Get testing accuracy
a_test <- cfm_test7$overall[1]
# Get number of nodes
nodes <- nrow(tree7$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 5, 50, 50))


# Tree 8
hyper8 = rpart.control(minsplit = 100, maxdepth = 5, minbucket = 100)
tree8 <- train(category ~., data=train_part, control = hyper8, trControl = train_control, method = 'rpart1SE')

# Training Set
# Evaluate the fit with confusion matrix
pred_tree8 <- predict(tree8, train_part)
# Confusion Matrix
cfm_train8 <- confusionMatrix(train_part$category, pred_tree8)

# Test Set
# Evaluate fit with a confusion matrix
pred_tree8 <- predict(tree8, test_part)
# Confusion Matrix
cfm_test8 <- confusionMatrix(test_part$category, pred_tree8)

#Get training accuracy
a_train <- cfm_train8$overall[1]
#Get testing accuracy
a_test <- cfm_test8$overall[1]
#Get number of nodes
nodes <- nrow(tree8$finalModel$frame)

# Add row to table
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 5, 100, 100))


# Tree 6
hyper6 = rpart.control(minsplit = 1000, maxdepth = 5, minbucket = 1000)
tree6 <- train(category ~., data = train_part, control = hyper6, trControl = train_control, method = "rpart1SE")

# Training Set
# Evaluate the fit with a confusion matrix
pred_tree6 <- predict(tree6, train_part)
# Confusion Matrix
cfm_train6 <- confusionMatrix(train_part$category, pred_tree6)

# Test Set
# Evaluate the fit with a confusion matrix
pred_tree6 <- predict(tree6, test_part)
# Confusion Matrix
cfm_test6 <- confusionMatrix(test_part$category, pred_tree6)

# Get training accuracy
a_train <- cfm_train6$overall[1]
# Get testing accuracy
a_test <- cfm_test6$overall[1]
# Get number of nodes
nodes <- nrow(tree6$finalModel$frame)

# Add rows to the table - Make sure the order is correct
comp_tbl <- comp_tbl %>% rbind(list(nodes, a_train, a_test, 5, 1000, 1000))


comp_tbl

```

```{r visualize table}

# Visualize with line plot
ggplot(comp_tbl, aes(x=Nodes)) + geom_line(aes(y = TrainAccuracy), color = "red", alpha = 0.5) + geom_line(aes(y = TestAccuracy), color="blue", alpha = 0.5) + ylab("Accuracy")

```

-   c: Identify the final choice of model, list it parameters and evaluate with a the confusion matrix to make sure that it gets balanced performance over classes. Also get a better accuracy estimate for this tree using cross validation.

```{r final choice model}
# train_control = trainControl(method = 'cv', numbers = 10)
final_hypers = rpart.control(maxdepth = 4, minsplit = 25, minbucket = 25)
final_tree <- train(category ~., data = train_part, trControl = train_control, control = final_hypers, method = 'rpart1SE')

train_predict <- predict(final_tree, train_part)
cfm_final_train <- confusionMatrix(train_part$category, train_predict)
cfm_final_train

test_predict <- predict(final_tree, test_part)
cfm_final_test <- confusionMatrix(test_part$category, test_predict)
cfm_final_test

```

With the max depth parameter of 4, minimum bucket and minimum split of 25, the prediction accuracy on the training and testing sets for this decision tree model was 100%. The similarity in performance on both sets of data indicate the model does not have an issue with over fitting. With these parameters, the decision tree had a total of 9 nodes.

## Problem 4

-   a: Build your initial decision tree model with minsplit=10 and maxdepth=20.

```{r initial decision tree}
#change hyper parameters
hyper_prob4 <- rpart.control(minsplit =  10, maxdepth = 20)

# train the model
tree_prob4 <- train(category ~., data = storms, control = hyper_prob4, trControl = train_control, method = "rpart1SE")

tree_prob4

```

-   b: Run variable importance analysis on the model and print the result.

```{r importance analysis}

# View the variable importance scores
var_imp <- varImp(tree_prob4, scale = FALSE)
print(var_imp)

```

-   c: Generate a plot to visualize the variables by importance.

```{r var imp plot}

# Summarize importance
plot(var_imp)

```

-   d: Rebuild your model with the top six variables only, based on the variable relevance analysis. Did this change have an effect on the accuracy?

```{r rebuild}

storms_6vars <- storms %>% select(c("category", "wind","pressure","hurricane_force_diameter", "lat", "tropicalstorm_force_diameter", "long"))

# train the model w/ top 6 vars
tree_prob4_b <- train(category ~., data = storms_6vars, control = hyper_prob4, trControl = train_control, method = "rpart1SE")

tree_prob4_b

nodes_prob4 <- nrow(tree_prob4_b$finalModel$frame)
nodes_prob4


```

With the use of only the top 6 variables, the model did not show any change on accuracy as it continued to have an accuracy rate of 100%.

-   e: Visualize the trees from (a) and (d) and report if reducing the number of variables had an effect on the size of the tree?

```{r visual tree}

fancyRpartPlot(tree_prob4$finalModel, caption = "Decision Tree for Storm Category")

fancyRpartPlot(tree_prob4_b$finalModel, caption = "Decision Tree for Storm Category with 6 Variables")
```

According to the decision trees that was trained from the entire data set and the data set with the top 6 important variables, there was no difference in performance, including regarding the size of the trees with the maximum depth parameter set to 20 and minimium split value of 10. These trees had a total of 9 nodes, including root, internal, and leaf nodes.
