---
title: "HW2"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

# HW 2 - DSC 441

## Problem 1

For this problem, you will load and perform some cleaning steps on a dataset in the provided BankData.csv, which is data about loan approvals from a bank in Japan (it has been modified from the original for our purposes in class, so use the provided version). Specifically, you will use visualization to examine the variables and normalization, binning and smoothing to change them in particular ways.

-   a: Visualize the distributions of the variables in this data. You can choose bar graphs, histograms and density plots. Make appropriate choices given each type of variables and be careful when selecting parameters like the number of bins for the histograms. Note there are some numerical variables and some categorical ones. The ones labeled as a ‘bool’ are Boolean variables, meaning they are only true or false and are thus a special type of categorical. Checking all the distributions with visualization and summary statistics is a typical step when beginning to work with new data.

```{r load data}
library(tidyverse)

loan = read_csv("/Users/danielkim/Downloads/BankData.csv")
summary(loan)
loan <- column_to_rownames(loan, var = "...1")
head(loan)

# drop na values
loan <- loan %>% drop_na()

loan$approval <- as.integer(loan$approval == "+")
loan[,c("bool1", "bool2", "bool3")] <- ifelse(loan[,c("bool1", "bool2", "bool3")] == "TRUE", 1, 0)

ggplot(loan, aes(credit.score)) + geom_histogram(binwidth = 25)
ggplot(loan, aes(approval)) + geom_bar()
ggplot(loan, aes(ages)) + geom_histogram(binwidth = 5)
ggplot(loan, aes(bool1)) + geom_bar()
ggplot(loan, aes(bool2)) + geom_bar()
ggplot(loan, aes(bool3)) + geom_bar()
ggplot(loan, aes(cont1)) + geom_density()
ggplot(loan, aes(cont2)) + geom_density()
ggplot(loan, aes(cont3)) + geom_density()
ggplot(loan, aes(cont4)) + geom_density()
ggplot(loan, aes(cont5)) + geom_density()
ggplot(loan, aes(cont6)) + geom_density()

```

-   b: Now apply normalization to some of these numerical distributions. Specifically, choose to apply z-score to one, min-max to another, and decimal scaling to a third. Explain your choices of which normalization applies to which variable in terms of what the variable means, what distribution it starts with, and how the normalization will affect it.

```{r normalization}
# z-score standardization
# formula: Z = (X – mean) / sd, where X is a single raw data value, mean is the population mean, and sd is the population standard deviation
copy_loan <- loan
mean_credit <- mean(copy_loan$credit.score)
sd_cred <- sd(copy_loan$credit.score)
copy_loan$credit.score <- (copy_loan$credit.score - mean_credit) / sd_cred 

# min-max normalization
# formula: (x - min(x)) / (max(x) - min(x)) * (new_max - new_min) + new_max, where x is a single raw data value
copy_loan$cont5 <- ((copy_loan$cont5 - min(copy_loan$cont5)) / (max(copy_loan$cont5) - min(copy_loan$cont5))) * (1 - 0) + 0

#decimal scaling
# formula: new_value = x/10i, where i is such that the max of |new_value| is less than 1
copy_loan$cont6 <- copy_loan$cont6 / 100000

```

I decided to perform a z-score standardization, or normalization, on the credit score attribute because the distribution for this variable was normal. The scaling would not have interfered with the mean or the distribution of the credit scores. I knew I wanted to scale both cont5 and cont6 as both these variables had data with large values relative to the other variables. As both variables did not have any data below 0, the decimal scaling and min-max normalization with a minimum value of 0 and maximum value of 1 scaled both variables to the same range (from 0 - 1). Therefore, I applied the min-max normalization to cont5 variable and decimal scaling to cont6 variable.

In all instances, the distribution of the attribute that was scaled did not change when compared to the distribution prior to the scaling. Additionally, the normalization scaled the range of values for all the attributes that was substantially different than the value range of the other variables. With the scale, there were less variables that would have more influence on the outcome of a model from having larger values or significant outliers.

-   c: Visualize the new distributions for the variables that have been normalized. What has changed from the previous visualization?

```{r normalized visualization}
# z-score standardization
ggplot(copy_loan, aes(credit.score)) + geom_histogram(binwidth = 1)

# min-max normalization
ggplot(copy_loan, aes(cont5)) + geom_density()

# decimal scaling
ggplot(copy_loan, aes(cont6)) + geom_density()

```

In all instances of normalization, the only difference to the visualization from the previous visualization was the data range of each variable, reducing all values from the thousands to the single digits.

-   d: Choose one of the numerical variables to work with for this problem. Let’s call it v. Create a new variable called v_bins that is a binned version of that variable. This v_bins will have a new set of values like low, medium, high. Choose the actual new values (you don’t need to use low, medium, high) and the ranges of v that they represent based on your understanding of v from your visualizations. You can use equal depth, equal width or custom ranges. Explain your choices: why did you choose to create that number of values and those particular ranges?

```{r bins}
v <- loan
v_bins <- v %>% mutate(credit.cat = cut(credit.score, breaks = c(300.00, 579.00, 669.00, 739.00, 799.00, 850.00), labels = c("Poor", "Fair", "Good", "Very Good", "Excellent")))
head(v_bins)
```

As many financial institution look at loan applicant's credit scores and determine whether the score is categorically good or bad, the credit score attribute was categorized into bins that reflected the score for that category. The credit.cat was created to store the bins for these scores. The particular ranges for each bin was pre-determined by common practices in the financial industry.

-   e: Building on (d), use v_bins to create a smoothed version of v. Choose a smoothing strategy to create a numerical version of the binned variable and explain your choices.

```{r bin smoothing}
# find out how many bins there are
ggplot(v_bins, aes(credit.cat)) + geom_bar()

fair <- v_bins %>% filter(credit.cat == "Fair") %>% mutate(credit.score = mean(credit.score, na.rm = TRUE))
good <- v_bins %>% filter(credit.cat == "Good") %>% mutate(credit.score = mean(credit.score, na.rm = TRUE))
v_good <- v_bins %>% filter(credit.cat == "Very Good") %>% mutate(credit.score = mean(credit.score, na.rm = TRUE))
excel <- v_bins %>% filter(credit.cat == "Excellent") %>% mutate(credit.score = mean(credit.score, na.rm = TRUE))

new_v_bins <- bind_rows(list(fair, good, v_good, excel))
head(new_v_bins)
```

For each of the existing bins, the mean score was determined and used to replace their credit score. This will not only smooth the bins, but also help visualize the average for each category.

## Problem 2

This is the first homework problem using machine learning algorithms. You will perform a straightforward training and evaluation of a support vector machine on the bank data from Problem 1. Start with a fresh copy, but be sure to remove rows with missing values first.

```{r new copy of dataset}
svm_data <- loan
head(svm_data)
```

-   a: Apply SVM to the data from Problem 1 to predict approval and report the accuracy using 10-fold cross validation.

```{r svm}
library(caret)
svm_data$approval <- as.factor(svm_data$approval)

cv_method <- trainControl(method = "cv", number = 10)
preproc <- c("center", "scale")

library(e1071)
cv_svm <- train(approval ~., data = svm_data, method = "svmLinear", trControl = cv_method, preProcess = preproc)
cv_svm
```

The accuracy of SVM was 86.3% using 10-fold cross validation.

-   b: Next, use the grid search functionality when training to optimize the C parameter of the SVM. What parameter was chosen and what is the accuracy?

```{r grid search}
# First we define the set of values to use for each parameter. SVM has only one: C.
grid <- expand.grid(C = 10^seq(-5, 1, 0.5))

# Fit the model with grid search
svm_grid <- train(approval ~., data = svm_data, method = "svmLinear", trControl = cv_method, tuneGrid = grid)
# View grid search result
svm_grid
```

The value for parameter C that was chosen was 0.00316 and this parameter value had an accuracy of 0.86485 with a kappa value of 0.73140.

-   c: Sometimes even if the grid of parameters in (b) includes the default value of C = 1 (used in (a)), the accuracy result will be different for this value of C. What could make that different?

The difference between the SVM accuracy from the default parameter value of 1 in the grid search vs. without the grid search can be explained by the difference in sample contained in each of the folds. As the folds in the grid search vs. without grid search may contain different set of samples, the model trained using different sets samples within each folds for the two scenarios (grid vs. non-grid).

## Problem 3

We will take SVM further in this problem, showing how it often gets used even when the data are not suitable, by first engineering the numerical features we need. There is a Star Wars dataset in the *dplyr* library. Load that library and you will be able to see it *(head(starwars))*. There are some variables we will not use, so first remove *films, vehicles, starships* and *name.* Also remove rows with missing values

```{r starwars data}
data(starwars)
star_data <- select(starwars, -c("films", "vehicles", "starships", "name"))
head(star_data)
summary(star_data)

# remove rows with missing values
star_data <- drop_na(star_data)
dim(star_data)
summary(star_data)
```

-   a: Several variables are categorical. We will use dummy variables to make it possible for SVM to use these. Leave the *gender* category out of the dummy variable conversion to use as a categorical for prediction. Show the resulting *head*.

```{r categorical dummy}
library(fastDummies)
star_data$sex_male <- ifelse(star_data$sex == "male", 1, 0)
star_data$sex_female <- ifelse(star_data$sex == "female", 1, 0)

dummy_star <- dummy_cols(star_data, select_columns = c("eye_color", "hair_color", "skin_color", "homeworld", "species"))
head(dummy_star)

```

-   b: Use SVM to predict gender and report the accuracy.

```{r starwars svm}
# remove non-numerical columns
new_star_data <- select(dummy_star, -c("eye_color", "hair_color", "skin_color", "homeworld", "species", "sex"))
head(new_star_data)

library(e1071)
train_control <- trainControl(method = "cv", number = 10)

star_svm <- train(gender ~., data = new_star_data, method = "svmLinear", trControl = train_control)
star_svm

```

The accuracy of the SVM was 91.66% with a kappa value of 0.7142.

-   c: Given that we have so many variables, it makes sense to consider using PCA. Run PCA on the data and determine an appropriate number of components to use. Document how you made the decision, including any graphs you used. Create a reduced version of the data with that number of principle components. **Note: make sure to remove gender from the data before running PCA because it would be cheating if PCA had access to the label you will use.** *Add it back in after reducing the data and show the result*.

```{r PCA}
# Under-represented columns might cause issues for PCA, called near zero variance problem. Any underrepresented category in a categorical variable might lead to this issue because they are represented as 1s and 0s. List of potential predictors as a set of column indices.
nzv <- nearZeroVar(new_star_data)
nzv
# new data for pca without near zero variance columns and target column
pca_star_data <- select(new_star_data, -c(nzv, "gender"))
head(pca_star_data)

# PCA
star_pca <- prcomp(pca_star_data)
summary(star_pca)

# Visualize the scree plot
screeplot(star_pca, type = "l") + title(xlab = "PCs")

# Create the components
preProc <- preProcess(pca_star_data, method="pca", pcaComp=3)
star_pc <- predict(preProc, pca_star_data)

# Put back target column
star_pc$gender <- as.factor(new_star_data$gender)

# New PCs are predictors for gender
(star_pc)
summary(star_pc)
```

The appropriate number of components to use was 3 components as 99.88% of the cumulative proportion of variance was explained by the first 3 components. The addition of each component after the first 3 components was marginal and did not provide significant value to the model. Additionally, as seen in the scree plot, the elbow of the curve was found at the 3rd component after which the curve significantly flattened.

-   d: Use SVM to predict *gender* again, but this time use the data resulting from PCA. Evaluate the results with a confusion matrix and at least two partitioning methods, using grid search on the C parameter each time.

```{r svm post pca}
grid_pca <- expand.grid(C = 10^seq(-5,2,0.5))

# k-fold cv model fit, train_control = trainControl(method = 'cv', number = 10)
pca_svm <- train(gender ~., data = star_pc, method = "svmLinear", trControl = train_control, tuneGrid = grid_pca)
pca_svm
pca_svm_predict <- predict(pca_svm, star_pc)
pca_svm_predict
confusionMatrix(star_pc$gender, pca_svm_predict)

# bootstrapping
train_control_boot = trainControl(method = "boot", number = 50)
# Fit the model
svm_boot <- train(gender ~., data = star_pc, method = "svmLinear", trControl = train_control_boot, tuneGrid = grid_pca)
svm_boot
svm_boot_predict <- predict(svm_boot, star_pc)
svm_boot_predict
confusionMatrix(star_pc$gender, svm_boot_predict)

# train/test splitting
# set the randomizer seed
set.seed(123)
# partition data
index = createDataPartition(y=star_pc$gender, p=0.7, list=FALSE)
# training set
train_set = star_pc[index,]
# testing set
test_set = star_pc[-index,]

# fit the model using the training set
svm_train <- train(gender ~., data = train_set, method = "svmLinear", tuneGrid = grid_pca)
svm_train
# predict with test set
pred_test <- predict(svm_train, test_set)

confusionMatrix(as.factor(test_set$gender), pred_test)
# manual quick calculation of accuracy: what proportion of predictions match labels
sum(pred_test == test_set$gender) / nrow(test_set)

```

I utilized 3 partitioning methods: 10-fold cross validation, bootstrapping, and train/tests splitting. In all instances of grid search, accuracy was used to select the optimal model parameter C using the largest accuracy value.

1.  10-fold CV: The final value used for the model was C = 1 with accuracy: 0.9655
2.  bootstrapping: The final value used for the model was C = 31.62278 with accuracy: 1
3.  test/train splitting: The final value used for the model was C = 10 with accuracy: 0.9298. The confusion matrix produced an accuracy of 0.8571.

-   e: Whether or not it has improved the accuracy, what has PCA done for the complexity of the model?

PCA is an unsupervised machine learning technique that reduced the number of dimensions in the star wars data set, transforming the original variables that are potentially correlated to each other into a smaller set of components (principal components). Although the interpretability of the variables are lost in the PCA transformation, the dimensional reduction decreases the complexity of the model while maintaining as much of the original information as possible.

## Bonus Problem

Use the Sacramento data from the caret library by running *data(Sacramento)* after loading caret. This data is about housing prices in Sacramento, California. Remove the *zip* and *city* variables.

```{r load Sacramento data}
library(caret)
data(Sacramento)
sac_data <- select(Sacramento, -c("zip", "city"))
head(sac_data)
summary(sac_data)
```

-   a: Explore the variables to see if they have reasonable distributions and show your work. We will be predicting the type variable – does that mean we have a class imbalance?

```{r explore}
ggplot(sac_data, aes(type)) + geom_bar()
ggplot(sac_data, aes(beds)) + geom_bar()
ggplot(sac_data, aes(baths)) + geom_bar()
ggplot(sac_data, aes(sqft)) + geom_density()
ggplot(sac_data, aes(price)) + geom_density()
ggplot(sac_data, aes(latitude)) + geom_density()
ggplot(sac_data, aes(longitude)) + geom_density()
```

As seen from the bar graph for the type categorical variable, there is a major imbalance in homes that are condo, multi-family, vs residential. There are significantly more residential properties than there are condos and multi-family homes. Multi-family homes have the fewest occurrences.

-   b: There are lots of options for working on the data to try to improve the performance of SVM, including (1) removing other variables that you know should not be part of the prediction, (2) dealing with extreme variations in some variables with smoothing, normalization or a log transform, (3) applying PCA, and (4) to removing outliers. Pick one now and continue.

```{r data process}
# remove variables that don't need to be part of the type prediction
rm_var <- select(sac_data, -c("latitude", "longitude"))
head(rm_var)

```

-   c: Use SVM to predict type and use grid search to get the best accuracy you can. The accuracy may be good, but look at the confusion matrix as well. Report what you find. Note that the kappa value provided with your SVM results can also help you see this. It is a measure of how well the classifier performed that takes into account the frequency of the classes.

```{r svm pca}
sac_control <- trainControl(method = 'cv', number = 10)
rm_var_svm <- train(type ~., data = rm_var, method = 'svmLinear', trControl = sac_control, tuneGrid = grid_pca)
rm_var_svm
rm_var_svm_predict <- predict(rm_var_svm, rm_var)
head(rm_var_svm_predict)
confusionMatrix(rm_var_svm_predict, rm_var$type)
```

The final value used for the model was C = 1 with accuracy: 0.9324. However, the kappa value for the parameter value of C = 1 was 0.1137. The low kappa value indicates the classifier type has 11.37% accuracy when taking into account the frequency of the predictor class, which in this case is type of home.

-   d: Return to (b) and try at least one other way to try to improve the data before running SVM again, as in (c).

```{r another preprocess}
# remove classifier type variable, PCA cannot run with classifier variable, PCA can only run with numeric variables
pca_sac_data <- select(rm_var, -c("type"))
head(pca_sac_data)
# remove any underrepresented variables, near zero variance attributes
nzv_sac <- nearZeroVar(pca_sac_data)
# no near zero variance issue
nzv_sac

# determine optimal components with PCA w/ cumulative variance
pca_sac <- prcomp(pca_sac_data)
summary(pca_sac)

# draw screeplot
screeplot(pca_sac, type = "l") + title(xlab = "PCs")

# preprocess data for PCA
sca_preProc <- preProcess(pca_sac_data, method = "pca", pcaComp = 2)
# reduce data into principal components
pca_sac_pc <- predict(sca_preProc, pca_sac_data)
# reintroduce type variable
pca_sac_pc$type <- rm_var$type
head(pca_sac_pc)

# train svm model with pca
pca_svm_sac <- train(type ~., data = rm_var, method = "svmLinear", trControl = sac_control, tuneGrid = grid_pca)
pca_svm_sac
pca_svm_sac_predict <- predict(pca_svm_sac, rm_var)
confusionMatrix(as.factor(pca_sac_pc$type), pca_svm_sac_predict)

```

-   e: In the end, some data are just so imbalanced that a classifier is never going to predict the minority class. Dealing with this is a huge topic. One simple possibility is to conclude that we do not have enough data to support predicting the very infrequent class(es) and remove them. If they are not actually important to the reason we are making the prediction, that could be fine. Another approach is to force the data to be more even by sampling.

Create a copy of the data that includes all the data from the two smaller classes, plus a small random sample of the large class (you can do this by separating those data with a filter, sampling, then attaching them back on). Check the distributions of the variables in this new data sample to make sure they are reasonably close to the originals using visualization and/or summary statistics. We want to make sure we did not get a strange sample where everything was cheap or there were only studio apartments, for example. You can rerun the sampling a few times if you are getting strange results. If it keeps happening, check your process.

```{r stratified data}
ggplot(sac_data, aes(type)) + geom_bar()
summary(sac_data$type)
multi <- filter(sac_data, type == "Multi_Family")
condo <- filter(sac_data, type == "Condo")
total_residential <- filter(sac_data, type == "Residential")
residential <- total_residential[sample(nrow(total_residential), size = 100), ]
head(residential)

#append database into one
new_data <- rbind(multi, condo, residential)
summary(new_data)

ggplot(new_data, aes(type)) + geom_bar()
ggplot(sac_data, aes(beds)) + geom_bar()
ggplot(sac_data, aes(baths)) + geom_bar()
ggplot(sac_data, aes(sqft)) + geom_density()
ggplot(sac_data, aes(price)) + geom_density()
ggplot(sac_data, aes(latitude)) + geom_density()
ggplot(sac_data, aes(longitude)) + geom_density()

```

Use SVM to predict type one this new, more balanced dataset and report its performance with a confusion matrix and with grid search to get the best accuracy.

```{r balanced svm}
new_train = trainControl(method = "cv", number = 10)
new_grid = expand.grid(C = 10^seq(-5,2,0.5))

new_svm <- train(type ~., data = new_data, method = "svmLinear", trControl = new_train, tuneGrid = new_grid)
new_svm
new_predict <- predict(new_svm, new_data)
head(new_predict)
confusionMatrix(new_data$type, new_predict)
```
The final value used for the model was C = 0.316227 with accuracy of 0.8382 and kappa of 0.6851. As a result of the confusion matrix, the accuracy of the predicted values was 0.8434 with a kappa value of 0.7. With a newly balanced classifier, the accuracy of the model dropped, however, the kappa value increased substantially. The trade-off in accuracy for increased kappa value is well worth the overall generalization while taking into consideration the frequency of the classifier classes.

## Bonus Problem

To understand just how much different subsets can differ, create a 5 fold partitioning of the cars data included in R *(mtcars)* and visualize the distribution of the *gears* variable across the folds. Rather than use the fancy *trainControl* methods for making the folds, create them directly so you actually can keep track of which data points are in which fold. This is not covered in the tutorial, but it is quick. Here is code to create 5 folds and a variable in the data frame that contains the fold index of each point. Use that resulting data frame to create your visualization.

```{r subsets}
mycars <- mtcars # make a copy to modify
mycars$folds = 0 # initialize new variable to hold fold indices
head(mycars)
# Create 5 folds, get a list of lists of indices.
# Take a look at this result so you understand what is happening.
# Note we are not passing the data frame directly, but a list of its indices created by 1:nrow(mycars). If you don’t understand how that works, try the individual parts on their own first
flds = createFolds(1:nrow(mycars), k=5, list=TRUE)
# This loop sets all the rows in a given fold to have that fold’s index in the folds variable. Take a look at the result and use it to create the visualization.
for (i in 1:5) { mycars$folds[flds[[i]]] = i}
head(mycars)

ggplot(mycars, aes(gear)) + geom_histogram(binwidth = 1) + facet_wrap(~folds)
```
