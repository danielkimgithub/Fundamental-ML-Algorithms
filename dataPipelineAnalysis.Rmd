---
title: "hw5project"
output: pdf_document
date: "2024-11-12"
---

# HW5 - DSC 441

## Single Problem - Entire Data Pipeline

### Goal: The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease.

#### a: Data gathering and integration

```{r data gathering and integration/import}
library(tidyverse)
obes <- read_csv("~/Downloads/ObesityDataSet.csv")
dim(obes)
addlObesityTrain <- read_csv("~/Downloads/addlObesity/train.csv")
dim(addlObesityTrain)

# First 5 rows of data sets
head(obes)
head(addlObesityTrain)

# Merge/Join two tables
obesity <- full_join(obes, addlObesityTrain)
# new obesity data set has identifying column
head(obesity) 

```

The data consist of the estimation of obesity levels in people from the countries of Mexico, Peru and Colombia, with ages between 14 and 61 and diverse eating habits and physical condition, the original data was collected using a web platform with a survey where anonymous users answered each question, then the information was processed obtaining 17 attributes and 2111 records.

Additional data was generated from a deep learning model trained on the original "Obesity or CVD risk" dataset (obes), which will be incorporated to the original dataset.

Data Source: [Obesity risk dataset](https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster)

#### b: Data cleaning

```{r data cleaning}
# Check for any NA values in dataset
sum(is.na(obesity))
summary(obesity) # NA values only in 'id' column

# remove ID column, which happens to also have NA values
obesity <- obesity %>% select(-c("id"))
# Double check no NA values
sum(is.na(obesity))

```

The attributes related with eating habits are: Frequent consumption of high caloric food (FAVC), Frequency of consumption of vegetables (FCVC), Number of main meals (NCP), Consumption of food between meals (CAEC), Consumption of water daily (CH20), and Consumption of alcohol (CALC). The attributes related with the physical condition are: Calories consumption monitoring (SCC), Physical activity frequency (FAF), Time using technology devices (TUE), Transportation used (MTRANS). All NA values were removed from the dataset, which also happened to be the identifying variable from the additional data set appended to the original dataset. All numeric variables were on similar scales, except for age and weight.

#### c: Data exploration

```{r Data exploration}
# Check class values
unique(obesity$NObeyesdad)
ggplot(obesity, aes(NObeyesdad)) + geom_bar()

# Scatter plot of Age, Weight, Height Against NObeyesdad
ggplot(obesity, aes(Age, Weight)) + geom_point(aes(color = NObeyesdad), alpha=0.5)
ggplot(obesity, aes(Height, Weight)) + geom_point(aes(color = NObeyesdad), alpha=0.5)
ggplot(obesity, aes(Height, Age)) + geom_point(aes(color = NObeyesdad), alpha=0.5)

```

As a result of the bar chart of the categorical target variable, the dataset contains equivalent number of each target class. It's difficult to distinguish any relationship between the target variable and obvious contributing factors to BMI classifications, such as age, weight, and height. There are 7 unique classes for the target NObeyesdad variable. It would be worthwhile for a feature engineer of BMI for each individual and discover any relationship between it and the target.

#### D: Preprocessing

```{r Data preprocessing}
library(ggplot2)
library(caret)
library(dplyr)
library(ggcorrplot)

# Feature Engineering: Calculate BMI from Height and Weight Variables
obesity$BMI <- obesity$Weight / (obesity$Height ** 2)
head(obesity)

# Boxplot/Violin Plot to Illustrate Distribution of BMI for each BMI Class
ggplot(obesity, aes(NObeyesdad, BMI)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.07) + coord_flip()

# Boxplot/Violin Plot to Illustrate Distribution of Age for each BMI Class
ggplot(obesity, aes(NObeyesdad, Age)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.07) + coord_flip()

# Dummy Variables for Binary Columns
obesity_num <- obesity
obesity_num[,c("SCC", "SMOKE", "FAVC", "family_history_with_overweight")] <- ifelse(obesity_num[,c("SCC", "SMOKE", "FAVC", "family_history_with_overweight")] == "yes", 1, 0)
obesity_num[,c("Gender")] <- ifelse(obesity_num[,c("Gender")] == "Male", 1, 0)
head(obesity_num)

# One Hot Encoding for Multi-Categorical variables
class <- data.frame(obesity_num$NObeyesdad)
head(class)
att <- obesity_num %>% select(-c("NObeyesdad"))
dummy <- dummyVars(" ~ .", data = att)
new_Obesity <- data.frame(predict(dummy, newdata = att))
final_Obesity <- cbind(new_Obesity, class)
head(new_Obesity)
head(final_Obesity)

# final_Obesity will used for model building, where all numeric variables are necessary
final_Obesity <- final_Obesity %>% rename(bmiCategory = obesity_num.NObeyesdad)
final_Obesity$bmiCategory <- as.factor(final_Obesity$bmiCategory)
head(final_Obesity)

# For data exploration, obesity data set will be utilized
# Mean BMI for Each Class
meanBMI <- obesity %>% group_by(NObeyesdad) %>% summarise(mean_bmi = mean(BMI))
head(meanBMI)
ggplot(meanBMI, aes(x = mean_bmi, y = NObeyesdad)) + geom_bar(stat = "identity", fill = 'steelblue', colour = 'black') + geom_text(aes(label = round(mean_bmi,2)), hjust = 2, color = "white", size = 4)

# Mean Age for Each Class
meanAge <- obesity %>% group_by(NObeyesdad) %>% summarise(mean_age = mean(Age))
ggplot(meanAge, aes(x = mean_age, y = NObeyesdad)) + geom_bar(stat = "identity", fill = 'steelblue', colour = 'black') + geom_text(aes(label = round(mean_age,2)), hjust = 2, color = "white", size = 4)

countPerClassByGender <- obesity %>% group_by(NObeyesdad) %>% group_by(Gender) %>% summarise(n = n())
head(countPerClassByGender)


# Visualize BMI Class by Gender
gender_class <- obesity %>% count(Gender, NObeyesdad) %>% rename(count = n)
ggplot(gender_class,aes(Gender, count, fill = NObeyesdad)) + geom_col(position = 'dodge') + geom_text(aes(label = count), position = position_dodge(0.9),  color = 'black', vjust = -0.5, size = 3)


# Mean BMI for Each Gender
meanBMIGender <- obesity %>% group_by(Gender) %>% summarise(mean_bmi = mean(BMI))
head(meanBMIGender)
ggplot(meanBMIGender, aes(x = Gender, y = mean_bmi)) + geom_bar(stat = "identity", fill = 'darkgreen', colour = 'black') + geom_text(aes(label = round(mean_bmi,2)), vjust = 5, color = "white", size = 4)

# Distribution of BMI by Gender
ggplot(obesity, aes(Gender, BMI, fill = Gender)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.07) + coord_flip()

# Correlation Matrix Heatmap
obesity_corr <- obesity %>% select_if(is.numeric)
ggcorrplot(cor(obesity_corr))


# Outlier Detection for all Numeric Variables (pre-dummyVars/OneHotEncoding)
ggplot(obesity, aes(Age)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Age") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(Height)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Height") + theme(plot.title = element_text(hjust=0.5)) 

ggplot(obesity, aes(Weight)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Weight") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(FCVC)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Frequency of consumption of vegetables (FCVC)") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(NCP)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Number of main meals (NCP)") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(CH2O)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Consumption of water daily (CH2O)") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(FAF)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Physical activity frequency (FAF)") + theme(plot.title = element_text(hjust=0.5))

ggplot(obesity, aes(TUE)) + geom_boxplot(fill = "#69b3a2") + 
  ggtitle("Boxplot of Time using technology devices (TUE)") + theme(plot.title = element_text(hjust=0.5))


# Check distribution of two extremely differently scaled variables to determine normalization process to perform
ggplot(final_Obesity, aes(Weight)) + geom_histogram(bins=30)
ggplot(final_Obesity, aes(Age)) + geom_histogram(bins = 30)

# For purposes of the Assignment 5 - Bin the Classes into 2 groups rather than the original 7 groups
# Two groups for purposes of this Assignment 5 will be "Normal" and "Obese"
twoClassData <- final_Obesity

# Min-Max Normalization
# formula: (x - min(x)) / (max(x) - min(x)) * (new_max - new_min) + new_max, where x is a single row data 
twoClassData$Age <- ((twoClassData$Age - min(twoClassData$Age)) / (max(twoClassData$Age) - min(final_Obesity$Age))) * (5 - 0) + 0
twoClassData$Weight <- ((twoClassData$Weight - min(twoClassData$Weight)) / (max(twoClassData$Weight) - min(twoClassData$Weight))) * (7 - 0) + 0


twoClassData[,c("bmiCategory")] <- ifelse(twoClassData[,c("bmiCategory")] %in% c("Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III", "Overweight_Level_I", "Overweight_Level_II"), "Obese", ifelse(twoClassData[,c('bmiCategory')] %in% c("Insufficient_Weight", "Normal_Weight"), "Normal", "Left Alone"))

twoClassData$bmiCategory <- as.factor(twoClassData$bmiCategory)

# Finalized data set with Two Classes
head(twoClassData)

# Check for near zero variance predictors after dummy/one-hot encoding
# (i.e., those variables that have very little variability or few unique values, having minuscule predictive power)
nzv <- nearZeroVar(twoClassData)

# nearZeroVar returns a list of indices
glimpse(nzv)
head(twoClassData[,c(nzv)])

# drop predictors with near zero variance
twoClassData <- twoClassData %>% select(,-c(nzv))
glimpse(twoClassData)

```

After the BMI variable was engineered, one hot encoding was performed for each categorical variable as well as generating dummy variables for each binary variables (i.e., yes/no, female/male). There were two variables of interest with values that were significantly different the value scales for the remaining variables. These variables were weight and age. The distribution for both these variables did not resemble anything close to that of a normal distribution. Therefore, a z-score normalization was not performed for standardization. Rather, a min-max normalization was performed for these variables, which also has the advantage of suppressing any outliers such as the ones seen in Age. Thus, all the variables in the data set were numeric variables and normalized. Finally, near zero variance predictors were removed from the cleaned and pre-processed dataset as these variables have minimal variability or unique values that would have any significance in classification of observations. The data set 'twoClassData', which has all numeric independent variables that have been normalized as well as factored dependent variable, will be used for model building.

We were able to highlight different relationships between the independent variables and the dependent variable. For example, male and female had similar average body mass indices, where the female BMI variance was greater than the male BMI variance. Additionally, Obesity Type III was most common for females while Obesity Type II was most common in males. The exploration into the dataset provides information that confirms the expectations from data about BMI and various health indicators. When looking at the bar chart for mean age and mean BMI to the classification, both mean age and mean BMI increases as the severity of the BMI classification increases. This exploration is expected. Lastly, there were not any concerning outliers or correlations involved with any of the independent variables as shown by the box plots and correlation matrix heat map.

```{r PCA}
twoClassData.predictors <- twoClassData %>% select(,-c("bmiCategory"))
twoClassData.target <- twoClassData %>% select("bmiCategory")

# Get PCA object with prcomp
bmi.pca <- prcomp(twoClassData.predictors)
# View the PCA summary with cumulative proportions
summary(bmi.pca)
# Visualize the scree plot
screeplot(bmi.pca, type = "l") + title(xlab = "PCs")


# Create the components
PCApreProc <- preProcess(twoClassData.predictors, method="pca", pcaComp=2)
bmi.pc <- predict(PCApreProc, twoClassData.predictors)
# Put back target column
bmi.pc$bmiCategory <- twoClassData.target$bmiCategory
# Make sure that we have the PCs as predictors
head(bmi.pc)

```

From the principal component analysis, the results show that 2 principal components capture most of the variance of roughly 95% variance. The second component provides the cumulative variance that exceeds 95%, supporting the use of 2 PCs to model the data.

#### Data Classification - Supervised

```{r Data Classification SVM Comparison Normal vs PC}
# By default cross validation doesn't use stratified sampling.
# Generate stratified indices (per fold list of indices, which are the row numbers)
idx <- createFolds(twoClassData$bmiCategory, 10, returnTrain = T)

# Train model evaluation method: cross-validation with 10-folds
cv_method_stratified <- trainControl(index = idx, method = "cv", number = 10)
# Scaling method
preproc <- c("center", "scale")


# SVM with Normal Data
svm_norm <- train(bmiCategory ~., data = twoClassData, method = "svmLinear", trControl = cv_method_stratified, preProcess = preproc)
svm_norm

# SVM with PC Data
svm_pc <- train(bmiCategory ~., data = bmi.pc, method = "svmLinear", trControl = cv_method_stratified, preProcess = preproc)
svm_pc

```

Although the principal components reduced the dimensionality of the data set, the full data set will be used moving forward, particularly as the dimensionality of the full data set was already reduced after the removal of near zero variance predictors. The removal of near zero variance predictors should also reduce likelihood of over fitting. The reduction in dimensionality and reduce the risk of over fitting was also the purpose of PCA. Therefore, the parameter for the SVM model will be tuned with the use of the normal data set instead of the PC data. It's also noting the normal data set provided a better model than the PC data set.

```{r twoClassData Test Train Split}
# train/test splitting
# set the randomize seed
set.seed(123)
# partition data
# By default, createDataPartition uses stratified random sampling under the hood.
index = createDataPartition(y=twoClassData$bmiCategory, p=0.7, list=FALSE)
# training set
train_set = twoClassData[index,]
# testing set
test_set = twoClassData[-index,]
```

In order to train and validate the model, the training and testing sets were created from the dataset.

```{r Data Classification SVM Parameter Tuning}
library(e1071) # grid search functionality
# Define the set of values to use for each parameter. SVM has only one: C.
set.seed(12)
grid <- expand.grid(C = 10^seq(-3, 1, 0.5))
cv_method <- trainControl(method = 'cv', number = 10, classProbs = TRUE)
# Fit the model with grid search
svm_grid <- train(bmiCategory ~., data = train_set, method = "svmLinear", trControl = cv_method, preProc = preproc, tuneGrid = grid)
# View grid search result
svm_grid


# Predict with train set
pred_train <- predict(svm_grid, train_set)
# calculation of accuracy: what proportion of predictions match labels
sum(pred_train == train_set$bmiCategory) / nrow(train_set)

# Predict with test set
pred_test <- predict(svm_grid, test_set)
# calculation of accuracy: what proportion of predictions match labels
sum(pred_test == test_set$bmiCategory) / nrow(test_set)

# Confusion Matrix for SVM Model
# train set
confusionMatrix(train_set$bmiCategory, pred_train)
# test set
confusionMatrix(test_set$bmiCategory, pred_test)

```

With the grid search to determine the best parameter, C, for the SVM model, the best parameter for model that resulted in the optimal accuracy of 0.9717 was C = 0.0316. Based on the predictions generated by the SVM model, the accuracy between the testing set and training set were similar, eliminating any concern for over fitting. The accuracy of predictions on these two sets were also 0.972.

```{r recreate dataset for use in DT model}
# Bin Class Category into Two Bins for Purposes of Assignment 5
twoClassDTData <- obesity
# Drop BMI variable, otherwise DT will split on BMI as BMI classification is dependent on BMI
twoClassDTData <- twoClassDTData %>% select(-c("BMI"))

# Create two bin target classification
twoClassDTData[,c("NObeyesdad")] <- ifelse(twoClassDTData$NObeyesdad %in% c("Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III", "Overweight_Level_I", "Overweight_Level_II"), "Obese", ifelse(twoClassDTData$NObeyesdad %in% c("Insufficient_Weight", "Normal_Weight"), "Normal", "Left Alone"))

# Factor Target Variable
twoClassDTData$NObeyesdad <- as.factor(twoClassDTData$NObeyesdad)
# Finalized data set with Two Classes
head(twoClassDTData$NObeyesdad)
```

Similar to the dataset created for the SVM model, where there were two bins for classification, the dataset for DT model was generated using the same procedures. However, this dataset did not remove any near zero variance predictors, did not create any dummy variables, and did not normalize variables. As decision trees can operate with categorical variables, the data set prior to one hot encoding (obesity) was used to train and tune parameters for this decision tree. The "obesity" data was also binned into two categories under similar classifications under the SVM model data set.

```{r obesity Test Train Split for DT}
# train/test splitting
# set the randomize seed
set.seed(94)
# partition data
# By default, createDataPartition uses stratified random sampling under the hood.
index_DT = createDataPartition(y=twoClassDTData$NObeyesdad, p=0.7, list=FALSE)
# training set
train_set_DT = twoClassDTData[index_DT,]
# testing set
test_set_DT = twoClassDTData[-index_DT,]
```

The training and testing sets were created from the 'twoClassDTData' set.

```{r Data Classification Decision Tree}
library(rpart)
train_control_DT = trainControl(method = "cv", number = 10)

# Fit the model
tree <- train(NObeyesdad ~., data = train_set_DT, method = "rpart1SE", trControl = train_control_DT)
tree

# Visualize this DT
library(rattle)
# Visualize your decision tree
fancyRpartPlot(tree$finalModel, caption = "")


# Training Set predictions
train_DT_pred <- predict(tree, train_set_DT)
# Testing Set predictions
test_DT_pred <- predict(tree, test_set_DT)

# Confusion Matrix for DT
# train set
confusionMatrix(train_set_DT$NObeyesdad, train_DT_pred)
# test set
confusionMatrix(test_set_DT$NObeyesdad, test_DT_pred)

# Check Variable Importance
importance <- varImp(tree, scale = FALSE)
plot(importance)
```

Based on the model generated, the DT had a maximum depth of 3 levels with a total of 11 nodes, where 4 of the nodes were terminal nodes. The visualization of the decision tree show us that the most impact variables were weight and height, specifically cutting the height at either 72 kg or 61 kg and the height at 1.6 meters. Most of the dataset was classified using the weight attribute as seen by nodes 4 and 3, carrying 20% and 68% of the total dataset respectively. We see that nine variables from the data set have direct relevance on the target variable as a result of the checking for variable importance in the DT. Finally, the accuracy of the decision tree on the test and training sets were similar with an accuracy of 0.96, confirming no overfitting was present.

```{r Data Classification DT Hyperparams Tuned}
# Set hyperparameters tunes
# Recall rpart1SE default parameters: minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxdepth = 30. 
hypers = rpart.control(maxdepth = 2, minsplit =  5000, minbucket = 2500)

# Fit the model
tree2 <- train(NObeyesdad ~., data = train_set_DT, control = hypers, trControl = train_control_DT, method = "rpart1SE")
fancyRpartPlot(tree2$finalModel, caption = "")


# Evaluate the fit with a confusion matrix
train_DT_pred2 <- predict(tree2, test_set_DT)
# Confusion Matrix
confusionMatrix(test_set_DT$NObeyesdad, train_DT_pred2)

```

Given a maximum level of 3 in the DT modeled, an attempt was made to tune parameters that would create a simpler model with less levels to determine if it would improve the accuracy of the model. The accuracy for the simplified DT model, where parameters were set to reduce complexity of the initial DT, was less accurate. Therefore, the initial DT model was kept. An attempt to try parameters that made a more complex DT was not necessary as the default parameters for the 'rsPart1SE' method gave the algorithm freedom to try more complex trees than the one that was created.

#### Data Classification Adv. Evaluation

```{r Data Classification Adv Evaluation}
library(pROC)
# 1)
# Confusion Matrix for SVM Model
# train set
train_results <- confusionMatrix(train_set$bmiCategory, pred_train)
# test set
test_results <- confusionMatrix(test_set$bmiCategory, pred_test)


# 2)
# Manual calculation of precision and recall: Positive instances for Normal Classification
# precision: Precision = TP / (TP + FP)
# recall: Recall = TP / (TP + FN)

# Precesion Recall for Train Set
train_results$table

precision_SVM_train <- 4085 / (4085+230)
precision_SVM_train
recall_SVM_train <- 4085 / (4085+204)
recall_SVM_train

# Precision Recall for Test Set
test_results$table

precision_SVM_test <- 1753 / (1753+96)
precision_SVM_test
recall_SVM_test <- 1753 / (1753+78)
recall_SVM_test

library(kernlab)
# 3) ROC plot
# Get class probabilities for decision tree model
pred_test_ROC <- predict(svm_grid, test_set, type = 'prob')
head(pred_test_ROC)

roc_obj <- roc((test_set$bmiCategory), pred_test_ROC[,1])
plot(roc_obj, print.auc=TRUE)

```

As the accuracy of the SVM model was greater than the accuracy of the DT, the confusion matrix for the SVM model was generated for this portion of the analysis/assignment. Additionally, the confusion matrix of for the SVM model illustrated no concern for over fitting as accuracy for testing and training sets were similar. Although these performance measures of my SVM classifier were lower than the accuracy of the model, the measures as a collective indicate the reliability of the model. Despite the lower performance measures, the precision and recall as well as the metrics from the confusion matrix show similar results, which show that the model is effective at predicting 'normal' vs. 'obese' individuals. The ROC plot also confirms the performance and effectiveness of the SVM classifier with an AUC of near 1. Thus, these performance measures make the classifier look favorable compared to the accuracy metric.

#### Data Clustering Model - Unsupervised

```{r Data Clustering Model}
library(factoextra)
library(stats)
# Use non-categorical variable data set without BMI variable 
# Data set has also been standardized from previous machine learning algorithm (SVM), data also needs to be numeric as cluster will be dependent on distances and dissimilarities
# Re-factor target variable from 2 factors to original 7 factors for purposes of clustering algorithm (2 factor classification was generated for purposes of classification algorithms)
kmean_data <- final_Obesity %>% select(-c("BMI"))
# Divide predictors vs target
kmean_target <- kmean_data %>% select(c("bmiCategory"))
kmean_vars <- kmean_data %>% select(-c("bmiCategory"))

# Normalize data to center around mean given cluster deals with distances
set.seed(30)
# Center scale allows us to standardize the data 
preproc_kmean <- preProcess(kmean_vars, method=c("center", "scale"))
# Fit data based on standardized preprocessing
kmean_vars <- predict(preproc_kmean, kmean_vars)

# Find number of clusters for k-means
fviz_nbclust(kmean_vars, kmeans, method = "wss")
fviz_nbclust(kmean_vars, kmeans, method = "silhouette")

# Fit the data
fit <- kmeans(kmean_vars, centers = 4, nstart = 25)
# Display the kmeans object information
fit

# Display cluster plot
fviz_cluster(fit, data = kmean_vars)

bmi_result <- data.frame(Status = kmean_data$bmiCategory, Kmeans = fit$cluster)

# Crosstab for Kmeans
bmi_result %>% group_by(Kmeans) %>% select(Kmeans, Status) %>% table()
```

Based on k-means clustering, the machine learning algorithm indicated the optimal number of clusters was 4. As seen in the wss and silhouette plots, the optimal number of clusters for the data was 4 clusters, which was used to fit the data into the k-means model. Given the original dataset, where there were 7 independent classes in the target variable, this makes sense as the major groups in the original dataset were: Insufficient Weight, Normal, Obesity Type I, Obesity Type II, Obesity Type III, Overweight Level I, and Overweight Level II. However, it would be reasonable to cluster these into Insufficient, Normal, Obese, and Overweight.

#### Report: Final Remarks

When looking at decision trees, support vector machines (SVM), and k-means clustering for a dataset on BMI classification, there are some fascinating insights to consider. Decision trees did well in their interpretability, making it easy to grasp how classifications are made based on factors like age, height, and weight. In this case, it was only the weight and height factors that played a major role in classification. The decision tree did a great job of capturing non-linear relationships in the data, and there wasn't any risk of over fitting as the tree was pruned. The analysis also conducted a search into potential simpler trees to ensure the outputted model was not too complex and ensure zero risk of over fitting the data.

SVMs focused on maximizing the margin between classes, in this case only two, which lead to better generalization when dealing with new data. Their advantage was drawing decision boundaries, especially in cases where BMI classes overlap. However, the interpretability isn't anything like that of decision tree. The SVM model did perform better in accuracy than the decision tree, but could be argued that this difference is marginal and might consider level of interpretability when deciding on a classification algorithm. 

K-means clustering was useful for uncovering natural groupings in the data without any pre-defined labels, or unsupervised learning. K-means performance relied heavily on the choice of features and the distance between two observations. The determination of the right number of clusters was obvious in this analysis, as seen by the silhouette and wss plots, however, there are times when determining the number of clusters is too difficult. The concern for the k-means model generated in this project was the lack of clarity in the clusters as seen in the cross tabulation of data points in clusters to their actual labels. 

In the end, comparing these models using metrics like accuracy, precision, and recall provides a well-rounded picture of their effectiveness. The model chosen often comes down to the specific context such as whether the priority is interpretability or predictive power. By combining these approaches, it provides a more nuanced understanding of BMI classification, leveraging each model's strengths to boost both accuracy and insights. Obviously in this case, it was a confirmation to see the models determine weight and height as the leading contributors to BMI classification. 

#### Reflection

The Foundations of Data Science course has transformed how I view the field. Initially, I thought of data science as just a technical area focused on algorithms and coding. However, the course emphasized the importance of storytelling, ethics, and domain knowledge, which opened my eyes to a more holistic approach. Iâ€™ve learned to blend statistical analysis, data visualization, and effective communication. Now, I see data science not just as a set of tools, but as a way to drive insights and tackle real-world challenges, and have already been thinking about how I can use these skills in varying fields. I am looking forward to applying this knowledge with Python.
