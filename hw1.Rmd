---
title: "hw1"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

# HW 1 - DSC 441

## Problem 1

-   a: First, we look at the summary statistics for all the variables. Based on those metrics, including the quartiles, compare two variables. What can you tell about their shape from these summaries?

```{r US Census Data}
library(tidyverse)

adult <- read_csv("/Users/danielkim/Downloads/FUNDAMENTALS OF DATA SCIENCE - 9232024 - 133 PM/adult.csv")
glimpse(adult)
dim(adult)
summary(adult)
```

Based on the results of the summary statistics for all the variables, the age of the US population had an unimodal, positively-skewed, or right skewed, distribution in which the difference between maximum age and 3rd quartile was roughly 40 years while the difference between the minimum age and 1st quartile was roughly 10 years. The number of years in education for the US population appears unimodal as well, but with symmetrical distribution with little-to-no skewness given the median and mode of variable are similar. Additionally, the difference between the 1st quartile and 3rd quartile to the minimum and maximum, respectively, are similar in value.

-   b: Use a visualization to get a fine-grain comparison (you don’t have to use QQ plots, though) of the distributions of those two variables. Why did you choose the type of visualization that you chose? How do your part (a) assumptions compare to what you can see visually?

```{r histogram age}
ggplot(adult, aes(age)) + geom_histogram(binwidth = 5)
```

```{r histogram education years}
ggplot(adult, aes(`education-num`)) + geom_histogram(binwidth = 1)
```

I decided to take advantage of the histogram visualizations to illustrate the distribution of these two numeric variables. The advantage of histograms over other distribution visualizations such as boxplots is the level of detail that can be added to the distribution plot with the parameters binwidth or bins. With these parameters, the histogram can take into account varying amounts of details for the distribution. When compared to my responses in part a) of the assignment, my assumptions were accurate in drawing the distribution for the age variable. However, the extent of this distribution was more severe than initially thought. In this instance, the distribution for the variable had a severe right-sided skewness with higher-valued outliers. The distribution for years of education was bimodal distribution with a mild left-sided skewness, most likely as a result of most students completing highschool and/or college.

-   c: Now create a scatterplot matrix of the numerical variables. What does this view show you that would be difficult to see looking at distributions?

```{r scatterplot}
ggplot(adult, aes(`education-num`, age)) + geom_point(alpha=0.5)
```

The scatter plots are generally useful to highlight any relationship between two numerical variables. Each point represents a sample or observation through which a discernible pattern can be detected, if any, when looking at data as a whole. In the instance above, the scatter plot illustrates no relationship between age and the number of years in school.

-   d: These data are a selection of US adults. It might not be a very balanced sample, though. Take a look at some categorical variables and see if any have a lot more of one category than others. There are many ways to do this, including histograms and following tidyverse group_by with count. I recommend you try a few for practice.

```{r cat}
ggplot(adult, aes(`marital-status`)) + geom_bar()
ggplot(adult, aes(relationship)) + geom_bar()
adult %>% group_by(sex) %>% select(sex, `marital-status`) %>% table()
adult %>% group_by(`native-country`) %>% summarise(count = n()) %>% head()
adult %>% group_by(`income-bracket`) %>% summarise(count = n())
```

-   e: Now we’ll consider a relationship between two categorical variables. Create a cross tabulation and then a corresponding visualization and explain a relationship between some of the values of the categorical.

```{r contingency plot}
ggplot(adult, aes(x=`income-bracket`, y=sex, fill = age)) + geom_tile()
adult %>% select(sex, `income-bracket`) %>% table()
```

## Problem 2

-   a: Join the two tables together so that you have one table with each state’s population for years 2010-2019. If you are unsure about what variable to use as the key for the join, consider what variable the two original tables have in common. (Show a head of the resulting table.)

```{r read two data tables}
library(dplyr)

evens <- read_csv("/Users/danielkim/Downloads/FUNDAMENTALS OF DATA SCIENCE - 9232024 - 133 PM/population_even.csv")
head(evens)

odds <- read_csv("/Users/danielkim/Downloads/FUNDAMENTALS OF DATA SCIENCE - 9232024 - 133 PM/population_odd.csv")
head(odds)

pop <- evens %>% inner_join(odds, by='STATE')
head(pop)
```

-   b: Clean this data up a bit (show a head of the data after):

    -   a: Remove the duplicate state ID column if your process created one.

    ```{r duplicate removed}
    pop <- pop %>% select(-c("NAME.y"))
    head(pop)
    ```

    -   b: Rename columns to be just the year number.

    ```{r rename}
    pop <- pop %>% setNames(c("STATE", "NAME", 2010, 2012, 2014, 2016, 2018, 2011, 2013, 2015, 2017, 2019))
    head(pop)
    ```

    -   c: Reorder the columns to be in year order.

    ```{r Reorder}
    pop <- pop %>% select(c("STATE", "NAME", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019"))
    head(pop)
    ```

-   c: Deal with missing values in the data by replacing them with the average of the surrounding years. For example, if you had a missing value for Georgia in 2016, you would replace it with the average of Georgia’s 2015 and 2017 numbers. This may require some manual effort.

```{r null values}
pop2 <- pop %>% select(-c("STATE")) %>% column_to_rownames(var = "NAME")
head(pop2)
transpose_pop2 <- as.data.frame(t(pop2))
transpose_pop2
summary(transpose_pop2) #used to id columns with null values
which(is.na(transpose_pop2)) #id location of null values

#fix null in Arizona
az_na <- transpose_pop2 %>% select(Arizona)
az_na
az_val <- sum(az_na[c("2010", "2012"),])/2
az_val
transpose_pop2$Arizona <- transpose_pop2$Arizona %>% replace_na(az_val)

#fix null in Idaho
id_na <- transpose_pop2 %>% select(Idaho)
id_na
id_val <- sum(id_na[c("2014", "2016"),])/2
id_val
transpose_pop2$Idaho <- transpose_pop2$Idaho %>% replace_na(id_val)

#fix null in Montana
mt_na <- transpose_pop2 %>% select(Montana)
mt_na
mt_val <- sum(mt_na[c("2016", "2018"),])/2
transpose_pop2$Montana <- transpose_pop2$Montana %>% replace_na(mt_val)

#fix null in Ohio
oh_na <- transpose_pop2 %>% select(Ohio)
oh_na
oh_val <- sum(oh_na[c("2012", "2014"),])/2
oh_val
transpose_pop2$Ohio <- transpose_pop2$Ohio %>% replace_na(oh_val)

#fix null in Wisconsin, replace null with mean of Wisconsin pop since no 2020 data available
wi_na <- transpose_pop2 %>% select(Wisconsin)
wi_na
transpose_pop2$Wisconsin <- transpose_pop2$Wisconsin %>% replace_na(mean(transpose_pop2$Wisconsin, na.rm = TRUE))

summary(transpose_pop2)
sum(is.na(transpose_pop2))
```

-   d: We can use some tidyverse aggregation to learn about the population.

    -   a: Get the maximum population for a single year for each state. Note that because you are using an aggregation function (max) across a row, you will need the rowwise() command in your tidyverse pipe. If you do not, the max value will not be individual to the row. Of course there are alternative ways.

    ```{r max pop}
    transpose_pop <- as.data.frame(t(transpose_pop2))
    transpose_pop

    max_pop <- transpose_pop %>% apply(1, max)
    max_pop
    ```

    -   b: Now get the total population across all years for each state. This should be possible with a very minor change to the code from (d). Why is that?

    ```{r Total Population}
    total_pop <- apply(transpose_pop, 1, sum)
    total_pop
    ```

    The total population across all years for each state has similar code from (d) because the minor change between these two codes is a change in aggregation function. Rather than using the max function in (d), the code for the total population across all years for each state uses the sum aggregation funtion.

-   e: Finally, get the total US population for one single year. Keep in mind that this can be done with a single line of code even without the tidyverse, so keep it simple.

```{r Total US Population}

max_US_pop <- apply(transpose_pop, 2, sum)
max_US_pop

```

## Problem 3

Continuing with the data from Problem 2, let’s create a graph of population over time for a few states (choose at least three yourself). This will require another data transformation, a reshaping. In order to create a line graph, we will need a variable that represents the year, so that it can be mapped to the x axis. Use a transformation to turn all those year columns into one column that holds the year, reducing the 10 year columns down to 2 columns (year and population). Once the data are in the right shape, it will be no harder than any line graph: put the population on the y axis and color by the state.

One important point: make sure you have named the columns to have only the year number (i.e., without popestimate). That can be done manually or by reading up on string (text) parsing (see the stringr library for a super useful tool). Even after doing that, you have a string version of the year. R is seeing the ‘word’ spelled two-zero-one-five instead of the number two thousand fifteen. It needs to be a number to work on a time axis. There are many ways to fix this. You can look into type_convert or do more string parsing (e.g., stringr). The simplest way is to apply the transformation right as you do the graphing. You can replace the year variable in the ggplot command with *as.integer (year)*.

```{r Population Timeline}
transpose_pop
pivot_long <- transpose_pop %>% rownames_to_column(var="State") %>% pivot_longer(cols = c("2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019"), names_to = "Year", values_to = "Population")
pivot_long$Year <- as.factor(as.integer(pivot_long$Year))

pivot_long

pivot_long_for_line <- filter(pivot_long, State == "California" | State == "Texas" | State == "Florida")

pivot_long_for_line

plt <- ggplot(pivot_long_for_line, aes(x=Year, y=Population, group = State)) + geom_line(aes(linetype = State, colour = State)) + geom_point(aes(shape = State, colour = State)) + ggtitle("State Population from 2010 - 2019") + theme(plot.title = element_text(hjust = 0.5))
plt
```

## Problem 4

-   a: Describe two ways in which data can be dirty, and for each one, provide a potential solution.

    Missing data and inconsistent formatting of data are two ways in which data can be dirty. Missing data occurs when data points are absent or missing. A potential solution to address missing data is to impute a statistical value for the missing value. These descriptive statistical values can vary, however, common statistics utilized are the mean or median. If the sample size is enough, the missing data, or null value, can be dropped from the data set. Inconsistent formatting of data occurs when data entries are not standardized. A common formatting inconsistency deals with text. For example, an entry can have "USA" vs. "U.S.A." Although these entries signify the same value, statistical analysis would produce different conclusions for "USA" and "U.S.A." The solution for this inconsistency is to normalize the data and standardize the formatting. Data normalization can also be applied to data with varying scale so that one variable with larger scale of measurement does not have more influence on the model or analysis than another variable with a smaller scale.

-   b: Explain which data mining functionality you would use to help with each of these data questions.

    -   a: Suppose we have data where each row is a customer and we have columns that describe their purchases. What are five groups of customers who buy similar things?

    As the question attempts to group customers who buy similar things into separate entities without pre-established classifications, the data mining functionality for cluster analysis should be used to help with this data question. The cluster analysis would help form groups that are not yet established based on their purchase history.

    -   b: For the same data: can I predict if a customer will buy milk based on what else they bought?

    As the question attempts to gain insight into the purchase habits of customers and predicting the likelihood of buying milk based on other purchases made, the data mining funcationality for classification should be used to help with this data question. Classification requires the prediction of an outcome into a category, in this case would be to determine whether a customer will purchase milk based on the their other purchases.

    -   c: Suppose we have data listing items in individual purchases. What are different sets of products that are often purchased together?

    As the question aims to uncover products that are often purchased together, the data mining functionality for association rule mining would be used to help with this data question. Association rule mining finds the events that occur together, in this case would be the sets of products that are purchased together.

-   c: Explain if each of the following is a data mining task

    -   a: Organizing the customers of a company according to education level.

    This is not a data mining task as the task is to simply reorganize the dataset according to a variable, the education level of customers. It does not provide any new actionable insight based on pattern discovery.

    -   b: Computing the total sales of a company.

    This is not a data mining task as computing the total sales of a company does not provide any valuable pattern that would help develop an actionable plan.

    -   c: Sorting a student database according to identification numbers.

    This is not a data mining task as the task is to sort, or organize, the dataset according to a variable, which is similar to the task asked in part (a).

    -   d: Predicting the outcomes of tossing a (fair) pair of dice.

    This is a data mining task, specifically a classification task as a model would need to be created to predict the likely outcomes of tossing a pair of dice. The model can provide general rules of thumb in games where winning is dependent on rolling a pair of dice.

    -   e: Predicting the future stock price of a company using historical records.

    This is a data mining task, specifically a time-series analysis as this model would be needed to predict the future stock price of a company. Based on historic prices, the time-series analysis would enable investors, shareholders, to understand the optimal time a company stock should be purchased or sold.
